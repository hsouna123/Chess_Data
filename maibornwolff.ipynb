{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae0e5f-9dac-47de-aeb6-d61770416a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.maibornwolff.de/en'\n",
    "\n",
    "# Make a GET request to the Maibornwolff website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e6ee7-0fc5-42f2-aafb-60f8507a7f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "# Set up the webdriver\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # Run Chrome in headless mode\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Load the Maibornwolff website\n",
    "url = 'https://www.maibornwolff.de/en'\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Find all the links on the page and extract their text and URLs\n",
    "links = [(a.text, a.get_attribute('href')) for a in driver.find_elements_by_tag_name('a')]\n",
    "\n",
    "# Print the links\n",
    "print(links)\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b3efa-5dda-47fe-a04c-2489aca1caa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Find all the paragraphs on the page and extract their text\n",
    "paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "\n",
    "# Find all the headings on the page and extract their text\n",
    "headings = [h.get_text() for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
    "\n",
    "# Find all the links on the page and extract their text and URLs\n",
    "links = [(a.get_text(), a['href']) for a in soup.find_all('a')]\n",
    "\n",
    "# Combine all the extracted data into a list of strings\n",
    "training_data = paragraphs + headings + [l[0] + ' ' + l[1] for l in links]\n",
    "\n",
    "# Print the training data\n",
    "print(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6713a-2cef-41c1-bad8-bc91d7409496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "url = 'https://www.maibornwolff.de/en'\n",
    "visited_urls = set()\n",
    "training_data = []\n",
    "\n",
    "def scrape_page(url):\n",
    "    # Make a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # If the request was successful and the page hasn't been visited before\n",
    "    if response.ok and url not in visited_urls:\n",
    "        # Mark the page as visited\n",
    "        visited_urls.add(url)\n",
    "        \n",
    "        # Parse the HTML content of the response using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all the paragraphs on the page and extract their text\n",
    "        paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "        \n",
    "        # Find all the headings on the page and extract their text\n",
    "        headings = [h.get_text() for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
    "        \n",
    "        # Find all the links on the page and extract their text and URLs\n",
    "        links = [(a.get_text(), urljoin(url, a['href'])) for a in soup.find_all('a')]\n",
    "        \n",
    "        # Combine all the extracted data into a list of strings\n",
    "        page_data = paragraphs + headings + [l[0] + ' ' + l[1] for l in links]\n",
    "        \n",
    "        # Add the page data to the training data\n",
    "        training_data.extend(page_data)\n",
    "        \n",
    "        # Recursively follow links on the page and scrape their content\n",
    "        for link in links:\n",
    "            scrape_page(link[1])\n",
    "\n",
    "# Scrape the Maibornwolff website starting from the homepage\n",
    "scrape_page(url)\n",
    "\n",
    "# Print the training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5719645-837a-4bf1-b39b-9023b6e2c1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc74d39-e754-4311-a180-fac662b54f31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f359112-c0f0-4ac8-a767-40fb535ac73c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"ya3tek3asba\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
